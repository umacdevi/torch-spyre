from typing import Optional
import torch

from .stickify import SpyreDCI, spyre_reduction_result_shape
from . import Unsupported


@torch.library.custom_op("spyre::compact", mutates_args=())
def compact(input: torch.Tensor) -> torch.Tensor:
    if len(input.size()) != 1:
        raise Unsupported("compact not implemented for 1-D tensors")
    return input.clone()


@compact.register_fake
def _(input):
    if len(input.size()) != 1:
        raise Unsupported("compact only implemented for 1-D tensors")
    output = input.new_empty(input.size())
    output.spyre_dci = SpyreDCI([0], stick_sparse=False)
    return output


@torch.library.custom_op("spyre::swap", mutates_args=(), device_types="spyre")
def swap(input: torch.Tensor) -> torch.Tensor:
    if len(input.size()) != 1:
        raise Unsupported("swap only implemented for 1-D tensors")
    output = input.new_empty_strided(input.size(), [64])
    output.spyre_dci = SpyreDCI([0], stick_sparse=True)
    return output


@swap.register_fake
def _(input):
    if len(input.size()) != 1:
        raise Unsupported("swap only implemented for 1-D tensors")
    output = input.new_empty_strided(input.size(), [64])
    output.spyre_dci = SpyreDCI([0], stick_sparse=True)
    return output


@torch.library.custom_op("spyre::slice", mutates_args=(), device_types="spyre")
def slice(input: torch.Tensor) -> torch.Tensor:
    if len(input.size()) != 1:
        raise Unsupported("slice only implemented for 1-D tensors")
    output = input.new_empty(input.size())
    output.spyre_dci = SpyreDCI([0], stick_sparse=False)
    return output


@slice.register_fake
def _(input):
    if len(input.size()) != 1:
        raise Unsupported("slice only implemented for 1-D tensors")
    output = input.new_empty(input.size())
    output.spyre_dci = SpyreDCI([0], stick_sparse=False)
    return output


@torch.library.custom_op("spyre::layer_norm", mutates_args=())
def layer_norm(
    input: torch.Tensor,
    normalized_shape: list[int],
    weight: Optional[torch.Tensor] = None,
    bias: Optional[torch.Tensor] = None,
    eps: float = 1e-5,
) -> torch.Tensor:
    if len(normalized_shape) != 1:
        raise Unsupported(
            f"spyre.layernorm: unsupported reduction shape {normalized_shape}"
        )
    return torch.native_layer_norm(input, normalized_shape, weight, bias, eps)[0]


@layer_norm.register_fake
def _(
    input: torch.Tensor,
    normalized_shape: list[int],
    weight: Optional[torch.Tensor] = None,
    bias: Optional[torch.Tensor] = None,
    eps: float = 1e-5,
):
    res = input.new_empty(input.size())
    res.spyre_dci = input.get_dci()
    return res


@torch.library.custom_op("spyre::exx2", mutates_args=(), device_types="spyre")
def exx2(input: torch.Tensor, exx2Scale: float, useZeroMean: bool) -> torch.Tensor:
    pass


@exx2.register_fake
def _(input: torch.Tensor, exx2Scale: float, useZeroMean: bool):
    res_size, res_dci = spyre_reduction_result_shape(input, [input.ndim - 1], True)
    res = input.new_empty(res_size)
    res.spyre_dci = res_dci
    return res


@torch.library.custom_op("spyre::layernormscale", mutates_args=(), device_types="spyre")
def layernormscale(input: torch.Tensor, eps: float) -> torch.Tensor:
    pass


@layernormscale.register_fake
def _(input: torch.Tensor, eps: float) -> torch.Tensor:
    res = input.new_empty(input.size())
    res.spyre_dci = input.get_dci()
    return res


@torch.library.custom_op("spyre::layernormnorm", mutates_args=(), device_types="spyre")
def layernormnorm(
    input: torch.Tensor,
    mean: torch.Tensor,
    norm_mean: torch.Tensor,
    weight: Optional[torch.Tensor],
    bias: Optional[torch.Tensor],
) -> torch.Tensor:
    pass


@layernormnorm.register_fake
def _(
    input: torch.Tensor,
    mean: torch.Tensor,
    norm_mean: torch.Tensor,
    weight: Optional[torch.Tensor],
    bias: Optional[torch.Tensor],
) -> torch.Tensor:
    res = input.new_empty(input.size())
    res.spyre_dci = input.get_dci()
    return res
