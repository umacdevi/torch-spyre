from typing import Optional
import torch
from torch._subclasses.fake_tensor import (
    FakeTensorMode,
    FakeTensor,
    FakeTensorConverter,
)
from torch._inductor.ir import FlexibleLayout, Layout, significant_strides_equal
from torch._subclasses.fake_impls import fast_detach
from .stickify import tensor_get_dci, SpyreDCI, SpyreFixedLayout

orig_from_real_tensor = FakeTensorConverter.from_real_tensor
orig_from_meta_and_device = FakeTensorConverter.from_meta_and_device


def install_spyre_tensors():
    """Extend Tensor and IR Classes for Spyre Stickification"""
    torch.Tensor.get_dci = tensor_get_dci
    FakeTensorConverter.from_meta_and_device = spyre_ftc_from_meta_and_device
    FakeTensorConverter.from_real_tensor = spyre_ftc_from_real_tensor
    torch._functorch._aot_autograd.dispatch_and_compile_graph._detach_and_copy_item_memo = spyre_detach_and_copy_item_memo
    torch.fx.experimental.proxy_tensor.snapshot_fake = spyre_snapshot_fake
    torch.fx.passes.fake_tensor_prop.snapshot_fake = spyre_snapshot_fake
    torch._inductor.ir.Buffer.get_layout = spyre_get_layout
    torch._inductor.ir.Buffer.freeze_layout_with_exact_strides = (
        spyre_freeze_layout_with_exact_strides
    )


def spyre_ftc_from_real_tensor(
    self,
    fake_mode,
    t: torch.Tensor,
    make_constant: bool = False,
    shape_env=None,
    *,
    source=None,
    symbolic_context=None,
    trace: bool = True,
) -> FakeTensor:
    res: FakeTensor = orig_from_real_tensor(
        self,
        fake_mode,
        t,
        make_constant=make_constant,
        shape_env=shape_env,
        source=source,
        symbolic_context=symbolic_context,
        trace=trace,
    )
    if t.device.type == "spyre":
        # TODO: Extract DCI from SpyreTensorImpl (once torch_spyre stores it).
        #       For initial development, synthesize a DCI that encodes generic stick layout.
        res.spyre_dci = SpyreDCI.generic_stick_dci(res)
    return res


def spyre_ftc_from_meta_and_device(
    self, fake_mode: FakeTensorMode, t: torch.Tensor, device: torch.device
) -> FakeTensor:
    res = orig_from_meta_and_device(self, fake_mode, t, device)
    if hasattr(t, "spyre_dci"):
        res.spyre_dci = t.get_dci()
    return res


def spyre_snapshot_fake(val: torch.Tensor) -> Optional[torch.Tensor]:
    if isinstance(val, FakeTensor):
        res = fast_detach(val.fake_mode, val)
    else:
        res = val.detach()
    # Propagate SpyreDCI to detached copy of val
    if res is not None and hasattr(val, "spyre_dci"):
        res.spyre_dci = val.spyre_dci

    return res


def spyre_detach_and_copy_item_memo(t):
    detached_t = t.detach()
    if hasattr(t, "item_memo"):
        detached_t.item_memo = t.item_memo
    if hasattr(t, "spyre_dci"):
        detached_t.spyre_dci = t.spyre_dci
    return detached_t


def spyre_get_layout(self: torch._inductor.ir.Buffer) -> Layout:
    if isinstance(self.layout, FlexibleLayout):
        for n in self.origins:
            t = n.meta.get("val", None)
            if isinstance(t, torch.Tensor):
                if t.device.type == "spyre":
                    dci = t.get_dci()
                    self.layout = dci.spyre_layout(t.device, t.size(), t.dtype)
                    return self.layout
            elif isinstance(t, tuple) and (
                n.target == torch.ops.aten.max.dim or n.target == torch.ops.aten.min.dim
            ):
                # TODO: This only works because Spyre implements amax/amin and doesn't implement argmax/argmin
                t = t[0]
                if t.device.type == "spyre":
                    dci = t.get_dci()
                    self.layout = dci.spyre_layout(t.device, t.size(), t.dtype)
                    return self.layout

        return self.layout
    elif isinstance(self.layout, Layout):
        return self.layout
    raise NotImplementedError(type(self.layout).__name__)


def spyre_freeze_layout_with_exact_strides(  # type: ignore[no-untyped-def]
    self, exact_strides, allow_padding=False
) -> None:
    if isinstance(self.layout, SpyreFixedLayout):
        assert significant_strides_equal(
            exact_strides, self.layout.stride, self.layout.size
        )
    else:
        assert isinstance(self.layout, FlexibleLayout)
        self.layout = self.layout.as_exact_strides(
            exact_strides, allow_padding=allow_padding
        )
